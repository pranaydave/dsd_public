{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d721ca6-0173-4266-bd4f-b4a3b02ea070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe, Image processing\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Deepleaning: PYTORCH\n",
    "import os\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#EXPLAINABLE AI: TORCHVISION AND GRAD CAM\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "#UI: WIDGETS\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import io\n",
    "# Display the result\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf350aa-242c-41ef-a0f7-c21c55152240",
   "metadata": {},
   "source": [
    "<h2>DATA EXPLORATION</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "383bde67-cd8b-4dc2-95c9-09cae5fea848",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '<ENTER_PARENT_DIRECTORY_NAME'\n",
    "dir_train_in_wildfire = main_dir+'/train/wildfire'\n",
    "dir_train_in_nowildfire = main_dir+'./train/nowildfire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d519ea1e-3915-475f-85b2-918acc65e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the directory\n",
    "\n",
    "def get_location(directory_path):\n",
    "    lst_items_out = []\n",
    "    lst_header_out = ['longitude','latitude']\n",
    "    try:\n",
    "        filenames = os.listdir(directory_path)\n",
    "        #print(\"Files and directories in '\", directory_path, \"':\")\n",
    "        for filename in filenames:\n",
    "            fname = filename.replace('.jpg','')\n",
    "            fname = fname.split(',')\n",
    "            longitude = fname[0]\n",
    "            latitude = fname[1]\n",
    "            lst_items_out.append([longitude,latitude])\n",
    "            #print(filename)\n",
    "    except FileNotFoundError:\n",
    "        print(\"The directory does not exist.\")\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied.\")\n",
    "\n",
    "    return lst_items_out, lst_header_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66affe8a-e1c7-4992-b5dc-45a6057257d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_items_train, lst_header_train = get_location(dir_train_in_wildfire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48fbac1-bcfc-4517-955f-b2a3f2d3f592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitide</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.79731</td>\n",
       "      <td>47.6256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-69.5572</td>\n",
       "      <td>51.9064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-79.11752</td>\n",
       "      <td>47.38459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-72.743</td>\n",
       "      <td>45.7617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-75.7764</td>\n",
       "      <td>45.4516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitide  latitude\n",
       "0  -75.79731   47.6256\n",
       "1   -69.5572   51.9064\n",
       "2  -79.11752  47.38459\n",
       "3    -72.743   45.7617\n",
       "4   -75.7764   45.4516"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(lst_items_train)\n",
    "df.columns = ['longitide','latitude']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3151ff65-6bb5-4000-9485-f4ad15da9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = main_dir+'/wildfire_locations.csv'\n",
    "df.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111b0bf-44a8-444e-b8f4-d54d57382780",
   "metadata": {},
   "source": [
    "<h2>Get geospatial visualisation with datajarvis.ai</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31099d40-59e6-4fe5-9184-4f90a7f82bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload CSV at datajarvis.ai\n",
    "#Prompt - give a geospatial heatmap with Longitide as Longitide and Latitude as Latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e1db37-7f20-49e1-ba26-11c518eea2d7",
   "metadata": {},
   "source": [
    "<h2>MODEL PYTORCH</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f768fb3-0fdd-4fec-8e21-31c187a4d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device.. mps\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print ('device..',device)\n",
    "# Paths\n",
    "data_dir = main_dir+\"/train\"  # Replace with your root data directory\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize all images to 128x128\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize images\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_data = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model (simple CNN)\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 32 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2)  # Output: 2 classes (wildfire, no wildfire)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c919aae7-52e3-4513-bf94-217a8772657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2057\n",
      "Epoch [2/10], Loss: 0.1498\n",
      "Epoch [3/10], Loss: 0.1269\n",
      "Epoch [4/10], Loss: 0.1077\n",
      "Epoch [5/10], Loss: 0.0878\n",
      "Epoch [6/10], Loss: 0.0684\n",
      "Epoch [7/10], Loss: 0.0543\n",
      "Epoch [8/10], Loss: 0.0511\n",
      "Epoch [9/10], Loss: 0.0407\n",
      "Epoch [10/10], Loss: 0.0384\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df3f204-32d6-41f5-ae3c-c7d45ff69718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "wildfire_model_path = main_dir+'/wildfire_model.pth'\n",
    "torch.save(model.state_dict(), wildfire_model_path)\n",
    "#print(\"Model saved as \"+wildfire_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e092631-7734-4750-874e-af58ad3248b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/mpmvnq3921n9j9rgk4zmcltw0000gp/T/ipykernel_82501/4270485815.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(wildfire_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "# Initialize the model\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = CNNModel().to(device)\n",
    "\n",
    "# Load model weights\n",
    "model.load_state_dict(torch.load(wildfire_model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Define preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to match training size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Same normalization as training\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfe219a-9eb9-4ab6-945c-60da30f5bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "# Function to predict wildfire or no wildfire\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')  # Load image\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Apply transformations and add batch dimension\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class index\n",
    "    \n",
    "    # Map class index to label\n",
    "    classes = {0: \"NO WILDFIRE RISK\", 1: \"WILDFIRE RISK\"}\n",
    "    return classes[predicted.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5a33e87-bc1b-474b-a636-02f9de66907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict images from a folder\n",
    "\n",
    "def read_image_and_predict(folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):  # Ensure it's a file\n",
    "            prediction = predict_image(file_path)\n",
    "            #print(f\"Image: {file_name} => Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "258fa805-15f7-43c8-a803-4a968574baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfire_path = main_dir+\"/wildfire/valid/wildfire\"  # Replace with the folder containing test images\n",
    "#read_image_and_predict(wildfire_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b733783-6f27-46ee-a0f4-b80462a841ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfire_path = main_dir+\"/valid/nowildfire\"  # Replace with the folder containing test images\n",
    "#read_image_and_predict(wildfire_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccfce89-6980-414e-957e-d3b72a4b8c52",
   "metadata": {},
   "source": [
    "<h2>Visualizations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ae469b3-4d6a-4047-9c95-71be3b213608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_prediction_and_explaination(image_path):\n",
    "    # Preprocessing transforms (adjust based on your model's requirements)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Resize to the input size of your model\n",
    "        transforms.ToTensor(),          # Convert to tensor\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize (same as during training)\n",
    "    ])\n",
    "\n",
    "    # Load and preprocess the input image\n",
    "#image_path = \"./valid/wildfire/-57.25,51.51.jpg\"  # Replace with your image path\n",
    "\n",
    "    prediction = predict_image(image_path)\n",
    "    print(f\"Image: {image_path} => Prediction: {prediction}\")\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')  # Open the image and ensure it's in RGB\n",
    "    input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    input_tensor = input_tensor.to(device)  # Move to the same device as the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Target layer for Grad-CAM (e.g., the first convolutional layer)\n",
    "    target_layer = model.conv_layers[0]  # Adjust based on your model structure\n",
    "\n",
    "    # Initialize Grad-CAM\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "    # Generate the Grad-CAM heatmap\n",
    "    grayscale_cam = cam(input_tensor=input_tensor)[0]  # [0] extracts the heatmap for the first image\n",
    "\n",
    "    # Convert the original image to a format compatible with `show_cam_on_image`\n",
    "    original_image = input_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()  # Convert to HWC format\n",
    "    original_image = (original_image - original_image.min()) / (original_image.max() - original_image.min())  # Normalize for display\n",
    "\n",
    "    # Overlay the heatmap on the original image\n",
    "    heatmap = show_cam_on_image(original_image, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    return prediction,heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0fbdb-04bd-42ee-b66e-cc128477f603",
   "metadata": {},
   "source": [
    "<b>UI</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eff7bc2d-a6e6-4289-99eb-053b6b61f6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileUpload(value=(), accept='image/*', description='Upload Image')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49065e4882c54dbb93f2bccb6122ba07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FileUpload(value=(), accept='image/*', description='Upload Image'), Button(description='Predict…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Directory to save uploaded images\n",
    "UPLOAD_DIR = \"uploaded_images\"\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# Function to handle image selection\n",
    "def on_image_upload(change):\n",
    "    # Get the uploaded image\n",
    "    uploaded_file = change['new']\n",
    "    if uploaded_file:\n",
    "        content = uploaded_file[0]['content']\n",
    "        img = Image.open(io.BytesIO(content))\n",
    "        img.thumbnail((300, 300))  # Resize for display\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            display(img)\n",
    "\n",
    "# Function to handle prediction\n",
    "def predict_image1(_):\n",
    "    \n",
    "    if image_upload.value:\n",
    "        # Simulate a prediction function\n",
    "        fname = image_upload.value[0]['name']\n",
    "        img_path = main_dir+'/predict/'+fname\n",
    "        print ('img_path..',img_path)\n",
    "        prediction,heatmap = get_prediction_and_explaination(img_path)\n",
    "        \n",
    "        result = \"Prediction: \"+prediction\n",
    "        with output:\n",
    "            display(\"Prediction complete!\")\n",
    "            display(result)\n",
    "\n",
    "          \n",
    "\n",
    "            plt.imshow(heatmap)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    else:\n",
    "        with output:\n",
    "            display(\"Please upload an image first.\")\n",
    "\n",
    "# Widgets\n",
    "image_upload = widgets.FileUpload(accept='image/*', multiple=False, description=\"Upload Image\")\n",
    "print (image_upload)\n",
    "image_upload.observe(on_image_upload, names='value')\n",
    "\n",
    "predict_button = widgets.Button(description=\"Predict\")\n",
    "predict_button.on_click(predict_image1)\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Display the UI\n",
    "ui = widgets.VBox([image_upload, predict_button, output])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a4564-a735-49c7-a084-bbc15476b9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
